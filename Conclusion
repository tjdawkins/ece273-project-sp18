Conclusion
Support Vector Machine is a convex problem that allows one to obtain a linear discrminant for a binary classifier. This problem can be solved in as the Lagrangian dual problem since strong duality holds. SVM works to maximize the margin between two data sets by orienting a hyperplane. By introducing a slack variable a model can be learned even from data which is not stricly linearly separable in it's home space. When working with non-seperable data a cost is assigned to slack variables which is a parameter of the learning process. The new objective function will be to maximize the marigin while minimizing the slack variables. As shown another feature of SVM is to use the kernel trick in the dual problem. In this way data separable by a non-linear discriminant can be mapped to a space where there is a hyperplane with constant norm to separate the data points. There is no need to know the transformation, but we assume an inner product on the transformed data. In this was we can find decision boundary for data which, in it's native space, is not linearly seperable. As shown this algorithm works well with the MNIST data set. Extension can be made to use SVM for multi-class by learning one vs all boundaries. 